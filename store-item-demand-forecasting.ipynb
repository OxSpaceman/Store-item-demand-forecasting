{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9999,"databundleVersionId":868225,"sourceType":"competition"}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Store Item Demand Forecasting\n\nYou are given 5 years of store-item sales data, and asked to predict 3 months of sales for 50 different items at 10 different stores.\n\n\n","metadata":{}},{"cell_type":"markdown","source":"## EDA\n\n","metadata":{}},{"cell_type":"code","source":"#####################################################\n# Demand Forecasting\n#####################################################\n\n\nimport time\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nimport warnings\n\npd.set_option('display.max_columns', None)\npd.set_option('display.width', 500)\nwarnings.filterwarnings('ignore')\n\n\ndef check_df(dataframe, head=5):\n    print(\"##################### Shape #####################\")\n    print(dataframe.shape)\n    print(\"##################### Types #####################\")\n    print(dataframe.dtypes)\n    print(\"##################### Head #####################\")\n    print(dataframe.head(head))\n    print(\"##################### Tail #####################\")\n    print(dataframe.tail(head))\n    print(\"##################### NA #####################\")\n    print(dataframe.isnull().sum())\n    print(\"##################### Quantiles #####################\")\n    print(dataframe.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)\n\n\n########################\n# Loading the data\n########################\n\ntrain = pd.read_csv('/kaggle/input/demand-forecasting-kernels-only/train.csv', parse_dates=['date'])\ntest = pd.read_csv('/kaggle/input/demand-forecasting-kernels-only/test.csv', parse_dates=['date'])\n\nsample_sub = pd.read_csv('/kaggle/input/demand-forecasting-kernels-only/sample_submission.csv')\n\ndf = pd.concat([train, test], sort=False)\n\n\n#####################################################\n# EDA\n#####################################################\n\ndf[\"date\"].min(), df[\"date\"].max()\n\ncheck_df(df)\n\ndf[[\"store\"]].nunique()\n\ndf[[\"item\"]].nunique()\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature Engineering","metadata":{}},{"cell_type":"code","source":"#####################################################\n# FEATURE ENGINEERING\n#####################################################\n\ndf.head()\n\ndef create_date_features(df):\n    df['month'] = df.date.dt.month\n    df['day_of_month'] = df.date.dt.day\n    df['day_of_year'] = df.date.dt.dayofyear\n    df['week_of_year'] = df.date.dt.isocalendar().week\n    df['day_of_week'] = df.date.dt.dayofweek\n    df['year'] = df.date.dt.year\n    df[\"is_wknd\"] = df.date.dt.weekday // 4\n    df['is_month_start'] = df.date.dt.is_month_start.astype(int)\n    df['is_month_end'] = df.date.dt.is_month_end.astype(int)\n    return df\n\ndf = create_date_features(df)\n\ndf.groupby([\"store\", \"item\", \"month\"]).agg({\"sales\": [\"sum\", \"mean\", \"median\", \"std\"]})\n\n\n########################\n# Random Noise\n########################\n\ndef random_noise(dataframe):\n    return np.random.normal(scale=1.6, size=(len(dataframe),))\n\n# 📌 Prevent Overfitting\n\n########################\n# Lag/Shifted Features\n########################\n\n\ndef lag_features(dataframe, lags):\n    for lag in lags:\n        dataframe['sales_lag_' + str(lag)] = dataframe.groupby([\"store\", \"item\"])['sales'].transform(\n            lambda x: x.shift(lag)) + random_noise(dataframe)\n    return dataframe\n\ndf = lag_features(df, [91, 98, 105, 112, 119, 126, 182, 364, 546, 728])\n\n\n\n# ⚠️ asked to predict 3 months \n\n########################\n# Rolling Mean Features\n########################\n\n\ndef roll_mean_features(dataframe, windows):\n    for window in windows:\n        dataframe['sales_roll_mean_' + str(window)] = dataframe.groupby([\"store\", \"item\"])['sales']. \\\n                                                          transform(\n            lambda x: x.shift(1).rolling(window=window, min_periods=10, win_type=\"triang\").mean()) + random_noise(\n            dataframe)\n    return dataframe\n\n\ndf = roll_mean_features(df, [365, 546])\n\n########################\n# Exponentially Weighted Mean Features\n########################\n\n\ndef ewm_features(dataframe, alphas, lags):\n    for alpha in alphas:\n        for lag in lags:\n            dataframe['sales_ewm_alpha_' + str(alpha).replace(\".\", \"\") + \"_lag_\" + str(lag)] = \\\n                dataframe.groupby([\"store\", \"item\"])['sales'].transform(lambda x: x.shift(lag).ewm(alpha=alpha).mean())\n    return dataframe\n\nalphas = [0.95, 0.9, 0.8, 0.7, 0.5]\nlags = [91, 98, 105, 112, 180, 270, 365, 546, 728]\n\ndf = ewm_features(df, alphas, lags)\n\n\n########################\n# One-Hot Encoding\n########################\n\ndf = pd.get_dummies(df, columns=['store', 'item', 'day_of_week', 'month'])\n\n\ndf.head()\n\n########################\n# Converting sales to log(1+sales)\n########################\n\ndf['sales'] = np.log1p(df[\"sales\"].values)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"#####################################################\n# Model\n#####################################################\n\n########################\n# Custom Cost Function\n########################\n\n# MAE, MSE, RMSE, SSE\n\n# MAE: mean absolute error\n# MAPE: mean absolute percentage error\n# SMAPE: Symmetric mean absolute percentage error (adjusted MAPE)\n\ndef smape(preds, target):\n    n = len(preds)\n    masked_arr = ~((preds == 0) & (target == 0))\n    preds, target = preds[masked_arr], target[masked_arr]\n    num = np.abs(preds - target)\n    denom = np.abs(preds) + np.abs(target)\n    smape_val = (200 * np.sum(num / denom)) / n\n    return smape_val\n\n\ndef lgbm_smape(preds, train_data):\n    labels = train_data.get_label()\n    smape_val = smape(np.expm1(preds), np.expm1(labels))\n    return 'SMAPE', smape_val, False\n\n\n########################\n# Time-Based Validation Sets\n########################\n\ntrain\ntest\n\n# till 2017 -> train\ntrain = df.loc[(df[\"date\"] < \"2017-01-01\"), :]\n\n# first 3 months of 2017 -> validation\nval = df.loc[(df[\"date\"] >= \"2017-01-01\") & (df[\"date\"] < \"2017-04-01\"), :]\n\ncols = [col for col in train.columns if col not in ['date', 'id', \"sales\", \"year\"]]\n\nY_train = train['sales']\nX_train = train[cols]\n\nY_val = val['sales']\nX_val = val[cols]\n\nY_train.shape, X_train.shape, Y_val.shape, X_val.shape\n\n########################\n# Time Series with LGBM\n########################\n\n# LightGBM parameters\nlgb_params = {'num_leaves': 10,\n              'learning_rate': 0.02,\n              'feature_fraction': 0.8,\n              'max_depth': 5,\n              'verbose': 0,\n              'num_boost_round': 10000,\n              'nthread': -1}\n\n\n\nlgbtrain = lgb.Dataset(data=X_train, label=Y_train, feature_name=cols)\n\nlgbval = lgb.Dataset(data=X_val, label=Y_val, reference=lgbtrain, feature_name=cols)\n\nmodel = lgb.train(lgb_params, lgbtrain,\n                  valid_sets=[lgbtrain, lgbval],\n                  num_boost_round=10000,\n                  callbacks=[lgb.early_stopping(2000),\n                             lgb.log_evaluation(500)],\n                  feval=lgbm_smape)\n\ny_pred_val = model.predict(X_val, num_iteration=model.best_iteration)\n\nsmape(np.expm1(y_pred_val), np.expm1(Y_val))\n\n\n########################\n# Feature Importance\n########################\n\ndef plot_lgb_importances(model, plot=False, num=10):\n    gain = model.feature_importance('gain')\n    feat_imp = pd.DataFrame({'feature': model.feature_name(),\n                             'split': model.feature_importance('split'),\n                             'gain': 100 * gain / gain.sum()}).sort_values('gain', ascending=False)\n    if plot:\n        plt.figure(figsize=(10, 10))\n        sns.set_theme(font_scale=1)\n        sns.barplot(x=\"gain\", y=\"feature\", data=feat_imp[0:25])\n        plt.title('feature')\n        plt.tight_layout()\n        plt.show()\n    else:\n        print(feat_imp.head(num))\n    return feat_imp\n\n\nplot_lgb_importances(model, num=30, plot=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Final Model ","metadata":{}},{"cell_type":"code","source":"########################\n# Final Model\n########################\n\ntrain = df.loc[~df.sales.isna()]\nY_train = train['sales']\nX_train = train[cols]\n\n\ntest = df.loc[df.sales.isna()]\nX_test = test[cols]\n\nlgb_params = {'num_leaves': 10,\n              'learning_rate': 0.02,\n              'feature_fraction': 0.8,\n              'max_depth': 5,\n              'verbose': 0,\n              'nthread': -1,\n              \"num_boost_round\": model.best_iteration}\n\nlgbtrain_all = lgb.Dataset(data=X_train, label=Y_train, feature_name=cols)\n\nfinal_model = lgb.train(lgb_params, lgbtrain_all, num_boost_round=model.best_iteration)\n\n\n\ntest_preds = final_model.predict(X_test, num_iteration=model.best_iteration)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"########################\n# Submission File\n########################\n\ntest.head()\n\nsubmission_df = test.loc[:, [\"id\", \"sales\"]]\nsubmission_df['sales'] = np.expm1(test_preds)\n\nsubmission_df['id'] = submission_df.id.astype(int)\n\nsubmission_df.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"validation SMAPE: 13.5273\ntraining   SMAPE: 12.7601","metadata":{}}]}